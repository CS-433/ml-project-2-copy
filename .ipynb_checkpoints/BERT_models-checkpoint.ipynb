{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to use google colab as it offers a P100 GPU that is fast and has with enough VRAM to train our model. Training and inference is very slow on CPU because of the size of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ialdTk_VEaYj"
   },
   "source": [
    "### Google Colab drive mount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uON2vVIjFJ1j"
   },
   "source": [
    "Change the following cell with the path of the folder in which you placed our github repository files in your google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8lghwN0EaYv"
   },
   "source": [
    "If loading this from google colab uncomment and run the following cell to mount your google drive folder to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fB5siqC7-h_g",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cO_QcXWFAA4"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/My\\ Drive/ML_proj2/GITHUB_SUBMISSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72zI8WmGEaYx"
   },
   "source": [
    "The following cell must be run if you are using google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aZ3xMEw2uo6n",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (4.14.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\younes\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\younes\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\ml\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\younes\\appdata\\roaming\\python\\python37\\site-packages (from sacremoses->transformers) (8.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running this notebook on your personal computer, we recommend you create a virtual environment (using anaconda) , and install all the required packages using these commands (this will fullfill the versions requirements on the README):  \n",
    "- conda create -y -n ml python=3.7 scipy pandas numpy matplotlib  \n",
    "- conda activate ml\n",
    "- conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch  \n",
    "- pip install -U scikit-learn  \n",
    "- conda install jupyterlab nb_conda_kernels  \n",
    "- pip install --user -U nltk  \n",
    "- conda install -c conda-forge ipywidgets\n",
    "- pip install transformers\n",
    "\n",
    "(note above that the virtual environment in named ml, and needs to be activated before installing the further dependeces and everytime before running this notebook)\n",
    "\n",
    "Python 3.7 was used for this as google COLAB currently uses it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM0a7JGuEaYx"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ltDR-NoHuj9v"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import RandomSampler, DataLoader, Subset\n",
    "from torch.utils.data import TensorDataset, random_split, SequentialSampler\n",
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "# https://stackoverflow.com/questions/42212810/tqdm-in-jupyter-notebook-prints-new-progress-bars-repeatedly\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell reloads .py files automatically into jupyter notebook even when saved outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JBTNRLUyEaY1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "djMrydNsEaY0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from preprocessing_bert import *\n",
    "from helpers_bert import *\n",
    "from models_bert import *\n",
    "from train_bert import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe9Eivv9JxBL"
   },
   "source": [
    "### run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnVL2JvrwJLm"
   },
   "source": [
    "If you are on google colab you can directly run run.py from here :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kGNcKAg-Jymk",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HNeBOrpq1ee"
   },
   "source": [
    "### Paths &  data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JgNliMqN_Rmc"
   },
   "outputs": [],
   "source": [
    "PATH_DATA = './data/'\n",
    "PATH_PREPROCESSING = PATH_DATA + 'preprocessing/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDZPYmFnEaY3"
   },
   "source": [
    "Loading the positive and negative tweets into dataframes, use the option small_dataset to load either the small dataset (1) or the full dataset (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bo59t8OQEaY3"
   },
   "outputs": [],
   "source": [
    "train_pos, train_neg = load_tweets(PATH_DATA, small_dataset=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q7JKNO1yEaY3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "device = gpu_cpu_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuBMC4wsEaY4"
   },
   "source": [
    "Below the input dataframes are correctly formatted and labeled then they are tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqmowWpKEaY4"
   },
   "outputs": [],
   "source": [
    "df = create_input_df(train_pos,train_neg)\n",
    "\n",
    "input_ids, attention_masks, labels = tokenize_with_autoencoder(df, max_len=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CTBh3ayEaY5"
   },
   "source": [
    "After that we create Dataloader objects for training and validation to be used by the training functions later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psFJmN2xEaY5"
   },
   "source": [
    "If you want to split the training data into a validation and test set use the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDZ4pTDSTEh-"
   },
   "outputs": [],
   "source": [
    "# with train test split\n",
    "full_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_ds, val_ds = train_val_split(full_dataset, proportion = 0.9)\n",
    "train_dataloader = DataLoader(train_ds, shuffle = True, batch_size = 32)\n",
    "val_dataloader = as_dataloader(val_ds, random = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RifeEjJ8EaY6"
   },
   "source": [
    "If you want to load only a subset of the dataset to see if the functions can run use the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xubCP4BOQOcl"
   },
   "outputs": [],
   "source": [
    "# Subset of train train test split -- To test if train function works\n",
    "full_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_ds, val_ds = train_val_split(full_dataset)\n",
    "train_dataloader = DataLoader(Subset(train_ds,np.arange(64*300)), shuffle = True, batch_size = 32)\n",
    "val_dataloader = as_dataloader(Subset(val_ds,np.arange(32)), random = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMHhNHDkEaY6"
   },
   "source": [
    "If you want to only train the model, use the following cell :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiBMHjpaJcWR"
   },
   "outputs": [],
   "source": [
    "# # Only train set\n",
    "train_dataloader = DataLoader(full_dataset, shuffle=True, batch_size = 16)\n",
    "val_dataloader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9o8Qi0AziMoT"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmmkajDMEaY7"
   },
   "source": [
    "Instantiating the model ( you can also instantiatite the modified model 'BertWithCustomClassifier' with a custom classifier )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_I6CoCBKdJYy"
   },
   "outputs": [],
   "source": [
    "model =  load_model(device, model_name = 'BertForSequenceClassification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1pCFeqwEaY7"
   },
   "source": [
    "Loading the optimizer and schdeuler needed for training : ( with the parameters for the best model BertForSequenceClassification )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ja6iz5MCKhH1"
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs # = number of batches times epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 3e-5, eps = 1e-8) # trying lr = 1e-5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps = round(total_steps*0.10), num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wed36G6mEaY8"
   },
   "source": [
    "Training the model (note that if using BertForSequenceClassification, do not use the option freezing=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKVLgA8VkGRF"
   },
   "outputs": [],
   "source": [
    "training_stats = train_bert_class_with_params(train_dataloader,val_dataloader,\n",
    "                                              model, optimizer, scheduler,\n",
    "                                              epochs, random_seed=42,\n",
    "                                              device=device,\n",
    "                                              PATH_DATA=PATH_DATA,\n",
    "                                              save_N_steps=599,\n",
    "                                              save_epoch=True,\n",
    "                                              save_path='./data/models/BERT/BERT_model_TEST',\n",
    "                                              step_print=100,\n",
    "                                              validate=True,\n",
    "                                              freezing=False,\n",
    "                                              freez_steps=100,\n",
    "                                              frozen_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a_vECrnEaY9"
   },
   "outputs": [],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjUITce6faJp"
   },
   "source": [
    "# Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaWF7VH3EaY9"
   },
   "outputs": [],
   "source": [
    "device = gpu_cpu_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OgobxA7EaY9"
   },
   "outputs": [],
   "source": [
    "# Our best model : \n",
    "path_model = PATH_DATA + 'models/BERT/best_submission_bert.pkl'\n",
    "model = load_model_disk(device, path_model, model_name = 'BertForSequenceClassification')\n",
    "\n",
    "# Our second best model : \n",
    "# path_model = PATH_DATA + 'models/BERT/best_submission_bert_custom.pkl'\n",
    "# model = load_model_disk(device, path_model, model_name = 'BertWithCustomClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGjio-mHEaY-"
   },
   "outputs": [],
   "source": [
    "path_test_data = PATH_DATA + 'twitter-datasets/test_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T6UpiDREaY-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use max_len=140 to reproduce our best results\n",
    "test_dataloader = load_test_data(path_test_data, max_len=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWMX0igvxHMN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred, ids = make_prediction(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nLMOjwMEaY_"
   },
   "outputs": [],
   "source": [
    "pred_sanity_checks(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Frc8sz2Aw6PM"
   },
   "outputs": [],
   "source": [
    "path_submission = PATH_DATA + 'submissions/output.csv'\n",
    "create_csv_submission(ids, y_pred, path_submission )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BaKUTzTEaY_"
   },
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dggt42H0EaZA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only use the small dataset for this, and a good gpu like a P100 otherwise it will probably take days to finish it\n",
    "# P100 also has approx 16 GB of VRAM, \n",
    "cv_bert(input_ids, attention_masks, labels, device, PATH_DATA, model_name = 'BertWithCustomClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-CL2Ky7EaZA"
   },
   "outputs": [],
   "source": [
    "cv_bert(input_ids, attention_masks, labels, device, PATH_DATA, model_name = 'BertWithCustomClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv(PATH_DATA+ '\\submissions\\output_run_py.csv' )\n",
    "test2 = pd.read_csv(PATH_DATA+ '\\submissions\\output_run_py_BEST_140len.csv' )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_2_BERT_models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
